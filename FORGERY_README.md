# Black Box Watermark Forgery Attack

This repository implements a black box watermark forgery attack as described in research on watermark transferability. The attack works by:

1. Taking a watermarked image
2. Running it through an exact inversion process to recover the latent representation
3. Generating a new image with a different prompt but using the recovered latent
4. Verifying if the watermark is still present in the new image

## Requirements

The code uses the same dependencies as the main PRC-Watermark repository. Make sure you have all the required packages installed.

For the analysis script, you'll also need:
- matplotlib
- pandas
- seaborn

## Usage

### Single Image Attack

To run the watermark forgery attack on a single image, use the `watermark_forgery.py` script:

```bash
python watermark_forgery.py \
  --input_image path/to/watermarked/image.png \
  --new_prompt "Your new prompt here" \
  --exp_id prc_num_10_steps_50_fpr_1e-05_nowm_0 \
  --original_model_id stabilityai/stable-diffusion-2-1-base \
  --proxy_model_id runwayml/stable-diffusion-v1-5 \
  --output_dir forgery_results
```

#### Arguments

- `--input_image`: Path to the watermarked input image (required)
- `--new_prompt`: New prompt for generating the forged image (required)
- `--exp_id`: Experiment ID for loading the watermark keys (required)
- `--original_model_id`: Model ID used for watermarking and inversion (default: 'stabilityai/stable-diffusion-2-1-base')
- `--proxy_model_id`: Proxy model ID used for generating the forged image (default: 'runwayml/stable-diffusion-v1-5')
- `--inf_steps`: Number of inference steps (default: 50)
- `--output_dir`: Output directory for forged images (default: 'forgery_results')
- `--var`: Variance for PRC watermark detection (default: 1.5)

### Batch Attack

To run the watermark forgery attack on multiple images, use the `batch_forgery.py` script:

```bash
python batch_forgery.py \
  --input_dir results/prc_num_10_steps_50_fpr_1e-05_nowm_0/original_images \
  --exp_id prc_num_10_steps_50_fpr_1e-05_nowm_0 \
  --original_model_id stabilityai/stable-diffusion-2-1-base \
  --proxy_model_id runwayml/stable-diffusion-v1-5 \
  --num_images 5 \
  --output_dir batch_forgery_results
```

#### Arguments

- `--input_dir`: Directory containing watermarked input images (required)
- `--exp_id`: Experiment ID for loading the watermark keys (required)
- `--dataset_id`: Dataset ID for prompts (default: 'Gustavosta/Stable-Diffusion-Prompts')
- `--original_model_id`: Model ID used for watermarking and inversion (default: 'stabilityai/stable-diffusion-2-1-base')
- `--proxy_model_id`: Proxy model ID used for generating the forged image (default: 'runwayml/stable-diffusion-v1-5')
- `--inf_steps`: Number of inference steps (default: 50)
- `--output_dir`: Output directory for forged images (default: 'batch_forgery_results')
- `--var`: Variance for PRC watermark detection (default: 1.5)
- `--num_images`: Number of images to process (default: 5)
- `--seed`: Random seed for reproducibility (default: 42)

### Analyzing Results

After running the batch forgery attack, you can analyze the results using the `analyze_forgery_results.py` script:

```bash
python analyze_forgery_results.py \
  --results_file batch_forgery_results/results.json \
  --output_dir batch_forgery_analysis
```

#### Arguments

- `--results_file`: Path to the results JSON file generated by the batch forgery script (required)
- `--output_dir`: Output directory for analysis results (default: same directory as results file)

The analysis script generates:
- Summary statistics of the attack success rate
- Visualizations including bar charts and pie charts
- A comparison grid of original and forged images
- A detailed text report of the results

## Example Workflow

1. First, generate watermarked images using the original `encode.py` script:

```bash
python encode.py --test_num 10 --method prc --model_id stabilityai/stable-diffusion-2-1-base
```

2. Then, run the batch forgery attack on the generated watermarked images:

```bash
python batch_forgery.py \
  --input_dir results/prc_num_10_steps_50_fpr_1e-05_nowm_0/original_images \
  --exp_id prc_num_10_steps_50_fpr_1e-05_nowm_0 \
  --num_images 5
```

3. Analyze the results of the attack:

```bash
python analyze_forgery_results.py --results_file batch_forgery_results/results.json
```

4. Check the output in the respective output directories and the analysis report to understand the effectiveness of the attack.

## How It Works

The attack exploits the fact that the watermark information is embedded in the latent space of the diffusion model. By inverting a watermarked image back to its latent representation and then using that latent to generate a new image (even with a different model), the watermark can potentially be preserved.

This demonstrates a vulnerability in watermarking schemes that rely solely on properties of the latent space, as the watermark can be transferred to new content without authorization.

## Notes

- The success of the attack may vary depending on the watermarking method, model architectures, and inversion quality.
- Using a proxy model that is significantly different from the original model may reduce the success rate of the attack.
- This implementation focuses on the PRC watermarking method, but the concept can be applied to other watermarking techniques as well.
- The batch attack script generates a JSON report with statistics on the success rate of the attack across multiple images.
- The analysis script provides visualizations and metrics to help understand the effectiveness of the attack. 