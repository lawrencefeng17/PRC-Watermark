# LLM Watermarking Project: Context & Discussion Summary

## Project Goal
Embed an undetectable, robust watermark into the output of Large Language Models (LLMs) using Pseudorandom Error-Correcting Codes (PRCs), primarily based on the ideas from "Pseudorandom Error-Correcting Codes" (Christ & Gunn, 2024 - henceforth CG24) and drawing inspiration from "Edit Distance Robust Watermarks for Language Models" (Golowich & Moitra - henceforth GM). The primary PRC construction used is LDPC-based.

## Core Algorithm Mechanics (Current Attempt: GM-Inspired Bucketing)

The current watermarking approach involves embedding one PRC bit per *original vocabulary token* generated by the LLM.

1.  **PRC Codeword Generation:**
    *   A zero-bit LDPC-PRC is used. Keys (`encoding_key`, `decoding_key`) are generated by `prc.KeyGen` (from `prc.py`).
    *   `encoding_key` includes a generator matrix `G`, a public one-time pad `z_pub` (referred to as `one_time_pad` in `prc.py`), and other parameters.
    *   `decoding_key` includes `G`, a sparse parity-check matrix `P` (where `PG=0`), `z_pub`, and other parameters.
    *   A PRC codeword `x_prc_binary` (length `n_prc_bits`, values {0, 1}) is generated:
        *   `X_pm1 = Encode(encoding_key)` from `prc.py`. This `X_pm1` is `Gu + e + z_pub` (internal noise `e`, pad `z_pub`), represented as {-1, 1}.
        *   `x_prc_binary = (1 - X_pm1) / 2` converts it to {0, 1}. **This is the target sequence to embed.**
    *   The pad `z_pub` was initially removed from `Encode` to simplify debugging, meaning `X_pm1` was `1 - 2*(Gu+e)`. This will need to be re-added for proper security.

2.  **Token-to-Bucket Hashing:**
    *   A fixed hash function `h: V -> {0, 1}` maps the entire LLM vocabulary `V` into two buckets (Bucket 0 and Bucket 1). This hash `h` (implemented as `self.token_hashes`) is precomputed and constant for a generation run.

3.  **Watermarked Token Generation (Iterative, per token):**
    *   **LLM Prediction & Top-p:** Get logits from LLM for the next token. Apply top-p sampling (parameter `p_top_p`) to get a reduced candidate set `V_top_p` and their renormalized probabilities `p'_llm`.
    *   **Pushforward Distribution:** Calculate the probability mass for Bucket 0 (`p_bar'(0)`) and Bucket 1 (`p_bar'(1)`) by summing `p'_llm(token)` for tokens in `V_top_p` that hash to 0 or 1 respectively.
    *   **Get PRC Bit:** Fetch the current PRC bit `x_j` from `x_prc_binary`.
    *   **`EmbedChar` (GM-Inspired):**
        *   Calculate `prob_branch = min(1, 2 * p_bar'(x_j))`. (Where `2` is `|Sigma_PRC|`).
        *   With probability `prob_branch`, set the target bucket `y_target = x_j`.
        *   Else (with probability `1 - prob_branch`), resample `y_target` from the "excess probability" distribution `q \propto [p_bar'(bucket_idx) - 0.5]_+`. This branch is the "rejection" step.
    *   **Final Token Sampling:** Sample the actual output token `t_output` from the LLM's distribution `p'_llm` (over `V_top_p`), conditioned on `h(t_output) = y_target`. (Effectively, zero out probabilities of tokens in `V_top_p` not in `Bucket_{y_target}` and renormalize).

4.  **Detection:**
    *   Given a sequence of generated tokens `(t_1, ..., t_L)`.
    *   Construct the "received" PRC bit sequence `x_received_binary` by hashing each token: `x_received_binary[k] = h(t_k)`.
    *   The detector uses `decoding_key` which contains `P` and `z_pub`.
    *   **Syndrome Calculation:** Calculate `y_eff = x_received_binary ^ z_pub` (where `z_pub` is the one-time pad portion from the PRC key, converted to {0,1}).
    *   Compute syndrome `S = (P @ y_eff) % 2` using GF(2) arithmetic.
    *   **Hamming Weight Check:** If `wt(S) < (0.5 - r**(-0.25)) * r`, declare watermarked. (`r` is number of rows in `P`).

## Relevant Theory

*   **LDPC Codes:** Defined by a sparse parity-check matrix `P`. A perfect codeword `c` satisfies `Pc=0`.
*   **PRC Pseudorandomness:** The output of `Encode(encoding_key)` (which is `Gu + e + z_pub`) should be computationally indistinguishable from a random string to an adversary knowing the public `G` and `z_pub` but not the secret `P`. This underpins undetectability.
*   **Syndrome Decoding (Simplified):** If `y_eff = c + e_total` (where `c` is perfect codeword, `e_total` is total noise), then `S = P@y_eff = P@e_total`. If `e_total` is sparse enough (low Hamming weight), then `wt(S)` will be significantly less than `r/2` (expected weight for random syndrome).
*   **GM `EmbedChar` Undetectability:** The GM algorithm is designed such that `E_{x_j}[P_final(token)] = P_original_llm(token)`, meaning averaged over a random PRC bit `x_j`, the output distribution matches the original.
*   **Embedding Noise:** An error occurs if `h(t_output) != x_j`. The probability of this error per token is `P_err = P(y_target != x_j)`. From GM, this error happens if the "rejection" branch is taken *and* `y_target` is resampled to `1-x_j`. This probability is `max(0, 1 - 2 * p_bar'(x_j))`.
    *   If `p_bar'(x_j)` (probability of target bucket) is high (e.g., >0.5), error probability is low.
    *   If `p_bar'(x_j)` is low (e.g., <0.5), error probability is `1 - 2*p_bar'(x_j)`, which can be high.

## Problems Solved / Progress Made

1.  **Bit-Level Embedding Noise Identified:** Realized that direct bit-level embedding (CG23 style) results in very high embedding noise (`e_emb`) because the conditional probability of the next bit (`hat_p_i`) is often near 0 or 1 due to LLM's low entropy and Huffman code structure. This overwhelms a weak PRC (e.g., `t=3`).
2.  **GM-Inspired Token-Level Bucketing Implemented:** This approach aims to embed one PRC bit per token by biasing bucket selection.
3.  **Successful Detection with Full-Length Generation & Specific Top-p:**
    *   With `top_p = 0.99` or `0.98`, and generating text long enough to match the PRC codeword length, the syndrome weight was well below the threshold (e.g., 311 vs threshold 689), indicating successful watermark embedding and detection.
    *   The "rejection rate" in `EmbedChar` was low (3-7%), suggesting the target PRC bit's bucket often had sufficient probability mass within the top-p set.
4.  **Quality Observation:** The initial ~100 tokens were coherent, but later text degenerated into gibberish. The hypothesis is that gibberish (flatter local distribution -> `p_bar'` closer to 0.5 -> lower embedding error) is easier to watermark reliably.

## Current Problems & Challenges

1.  **Quality vs. Watermarkability with Top-p:**
    *   The scheme is highly sensitive to the `top_p` parameter.
    *   `top_p=0.99/0.98`: Coherent start, then gibberish, but watermarked successfully.
    *   `top_p=0.97`: Watermark embedding fails (syndrome weight ~r/2). This suggests that with more restrictive top-p, `V_top_p` becomes too small/skewed, forcing frequent embedding errors if the PRC bit requires sampling from a bucket that has no (or very low probability) tokens in `V_top_p`.
    *   **Core Challenge:** Finding a balance. We need `V_top_p` to be diverse enough for `EmbedChar` to operate with low error (i.e., both `p_bar'(0)` and `p_bar'(1)` should ideally have decent mass if possible), but restrictive enough for quality. The current GM `EmbedChar` still makes hard choices (sampling *only* from the chosen bucket `y_target` within `V_top_p`).
2.  **Watermarking Coherent Text:** The primary goal is to reliably watermark the coherent, useful part of the LLM output. The fact that gibberish is easier to watermark is interesting but doesn't solve the main problem. The first ~100 coherent tokens *are* being watermarked sufficiently in the successful cases, but the sensitivity to `top_p` shows this is fragile.
3.  **Early EOS & Short Text Detection:**
    *   If generation stops due to EOS before `n_prc_bits` worth of *hashed token bits* are produced, the current detector cannot be directly applied.
    *   Solutions like padding are problematic. The best current approach is to declare "insufficient data" if the text is too short. This limits applicability for tasks requiring short responses.
4.  **Theoretical Undetectability vs. Practical Quality:** While GM `EmbedChar` is theoretically undetectable when averaged over random PRC bits, using a *fixed* PRC sequence can lead to quality degradation if it repeatedly forces the LLM off its high-probability path due to bucket restrictions. The top-p filtering mitigates this to some extent but hasn't fully solved it if very high quality *and* robustness are needed simultaneously.
5.  **PRC Robustness (`t` parameter):** It's still unclear if `t=3` is truly sufficient, even with the GM bucketing and top-p. The successful detection at `top_p=0.99` might be because the *average* error rate over the whole (partially gibberish) sequence was low enough. The coherent part might still have a higher error rate. Increasing `t` (e.g., to 15-20) would make the PRC more tolerant to errors, potentially allowing for more aggressive (lower `p`) top-p settings while still embedding successfully.

## Tractable Steps for Coding Tasks

*   **Parameter Sweeping for `t` and `top_p`:** Systematically explore the relationship between PRC sparsity `t` (in `KeyGen`, with `message_length=0`), `top_p`, generated text quality, and watermark detection rate (syndrome weight).
*   **Quantify Embedding Noise:** Track `P(h(sampled_token) != prc_bit_x_j)` for different `top_p` values to understand the actual noise the PRC needs to correct.

## Other Next Steps

*   **Handling Short Text Detection:** Implement a clear strategy for when `len(hashed_tokens) < n_prc_bits`. For now, this might just be an error or a "no detection" result.